---
title: "NML502 - Final Project"
author: "Elliot Smith, Eugen Hruska, Varun Suriyanarayana"
date: "4/22/2018"
output: pdf_document
---

```{r, echo = FALSE}

library(gridExtra)
library(png)
library(grid)
library(knitr)

suppressMessages(
    library(dplyr)
)

```

```{r, echo = FALSE}

##### Load the Data #####



## Import the data

data <- read.csv("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/recipeData.csv")

## Select my beer styles and variables of interest

selected_styles <- c("American IPA",
                     "American Pale Ale",
                     "Saison",
                     "American Light Lager",
                     "American Amber Ale",
                     "Imperial IPA",
                     "American Stout",
                     "Irish Red Ale",
                     "American Brown Ale",
                     "Witbier")

data_final <- data %>%
    filter(Style %in% selected_styles) %>%
    select(Style, Size.L., OG, FG, ABV, IBU, Color, BoilSize, BoilTime, BoilGravity, Efficiency)

data_final$Style <- factor(data_final$Style)

## Remove rows with NAs

data_final[data_final == "N/A"] <- NA
data_final <- data_final[complete.cases(data_final), ]

data_final$BoilGravity <- as.numeric(data_final$BoilGravity)

## Eugen's data scaling method

data_final_orig <- data_final

for (i in 2:11) {
    
    data_final[, i] <- (data_final_orig[, i] - (mean(data_final_orig[, i]) - 2*sd(data_final_orig[, i]))) /
        ((mean(data_final_orig[, i]) + 2*sd(data_final_orig[, i])) - (mean(data_final_orig[, i]) - 2*sd(data_final_orig[, i])))
    data_final[data_final[, i] > 1, i] <- 1
    data_final[data_final[, i] < 0, i] <- 0
    
}

for (i in 2:11) {
    
    data_final[, i] <- (data_final[, i] - (mean(data_final[, i]) - 2*sd(data_final[, i]))) /
        ((mean(data_final[, i]) + 2*sd(data_final[, i])) - (mean(data_final[, i]) - 2*sd(data_final[, i])))
    data_final[data_final[, i] > 1, i] <- 1
    data_final[data_final[, i] < 0, i] <- 0
    
}

## Subset the data for testing purposes

# data_final <- data_final[seq(1, nrow(data_final), 20), ]

## Reset the row names

rownames(data_final) <- NULL

## Separate the input space from the labels

output_space <- matrix(data_final$Style, nrow = length(data_final$Style))
input_space <- matrix(as.numeric(unlist(data_final[, 2:dim(data_final)[2]])), nrow = nrow(data_final[, 2:dim(data_final)[2]]))

## Define some data details

input_size <- dim(input_space)[2]
matrix_dim <- 15


########## Perform Analysis - K Means ##########



## Find the smallest data size

min_data_size <- min(table(data_final$Style))

## Take an even sample size of each Style

clust_plot <- rbind(
    sample_n(data_final[data_final$Style == "American IPA", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "American Pale Ale", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "Saison", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "American Light Lager", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "American Amber Ale", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "Imperial IPA", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "American Stout", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "Irish Red Ale", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "American Brown Ale", ], size = min_data_size, replace = FALSE),
    sample_n(data_final[data_final$Style == "Witbier", ], size = min_data_size, replace = FALSE)
)

## Calculate the K Means Clusters

kmeans_data <- kmeans(x = clust_plot[, 2:11], centers = 10)

## Calculate the misclassification rate

kmeans_rate_df <- data.frame(true_style = clust_plot$Style, k_clust = kmeans_data$cluster)
clust_table <- table(kmeans_rate_df)

totals <- apply(clust_table, 2, sum)
perc_clust <- setNames(data.frame(matrix(ncol = dim(clust_table)[1], nrow = 10)), colnames(clust_table))
rownames(perc_clust) <- rownames(clust_table)

for (i in 1:dim(clust_table)[2]) {
    
    perc_clust[, i] <- round(matrix(clust_table[, i]) / totals[i], 4)
    
}

```

# Introduction

In this report, we will be comparing the clustering results of a dataset of different beer types using two distinct methods, K Means and Self-Organizing Maps. Our previous experience tells us that when data reaches a particular level of complexity that K Means is no longer an acceptable method for clustering; we hope to prove or debunk this assertion. In addition, given our hypothesis is correct, that K Means is not a satisfactory method, we hope to prove that using an SOM network is a significantly stronger technique for identifying clusters in multi-dimensional data. We hope that you will learn something through this process as we have.

# Motivation

There were many motivations for us to perform this analysis:

* We love beer! While trying to decide on a project topic and looking for similar interests, we found that each of us enjoyed beer and that trying to cluster multi-dimensional beer data would be both interesting and fun; it certainly made working on the project very engaging as we were curious to test the results of our analysis against our previous experience.

* Our group had a strong preference to explore Unsupervies Learning (as opposed to a Supervised approach). As a team, we were certainly more interested in the concept of trying to find the solution without providing the answer; the potential power of accurate prediction without supervision was very interesting to us. However, having the answers available to us was also important, so that we could test how well our analyses performed.

* Self-Organizing Maps is a topic that deserved further exploration. We decided that an SOM was a robust tool that was perhaps best equipped to solve our problem with an optimal solution and we also had an intrinsic desire to explore the network's abilities more in a multi-dimensional setting; this project represented the perfect opportunity.

* Each of us had the desire to pursue the topic of classification; it worked to our benefit that the data we chose was very well-suited to our task. The concept that we can try to determine a observations class (in this case, a style of beer) by developing a learning algorithm was a major driver for us. Classification power is an extremely robust topic; the ability to classify observations based on their features is a topic worth exploring!

* Finally, we wanted to compare how Self-Organizing Maps compared against a more conventional clustering method (in this case, K Means). Since SOMs are a relatively new topic to us, we wanted to understand how it would compare to methods that we had more familiarity with in an effort to expand our statistical toolsets and perhaps replace our primitive tools (K Means) with a more robust one (SOMs).


# Objectives

Our objectives with this analysis are the following:

* Compare visually how effective K Means and SOMs are at clustering the data. We want to get a visual representation of how the two methods cluster the data in an effort to determine their efficiency at completing this task.

* Compare the successful classification rates for K Means and SOMs to determine which clustering performed better when we take into account the class labels. This simple number is our key statistic, it tells us that given our method used, we expect to correctly classify a particular percent of inputs.

* Determine if any subclasses exist within our defined classes based on the results of the clustering analysis. This result will work both ways in that it will help us understand how the clustering is realized, in addition to helping us understand differences internal to each class.

* Finally, we want to learn something new about beer that we did not know beforehand!

# Our Data

## Beer Data

Our data came from the kaggle.com website, a repository for publicly available data for personal or research analyses. The original data came with 73,861 observations and 23 distinct features. We decided to investigate the following beer styles:

* American Pale Ale
* Imperial IPA
* American IPA
* Saison
* American Brown Ale
* Witbier
* American Amber Ale
* Irish Red Ale
* American Stout
* American Light Lager

In addition, we decided to investigate the impacts of the following features:

* Size (L) - the amount brewed for each observation
* OG - the original density of the wort (key ingredient in the brewing process) compared to the water before fermenation
* FG - the final density of the wort compared to the water after fermentation
* ABV - alcohol by volumne
* IBU - international bittering units (an international standard for the beer's level of bitterness)
* Color - a numerical scale representing light to dark beer coloring
* BoilSize - the amount of fluid at the beginning of the boil process
* BoilTime - the amount of time the wort is boiled
* BoilGravity - density of the wort compared to the water before the boil
* Efficiency - how efficient the mash was at extracting sugar from the grain

Our reduction down to these selected beer styles (and after removing any rows that had any of the selected variables marked as NA) left us with 31,419 observations.

## Raw Data Issues

A preliminary look at our raw data told us that each of our features were heavily affected by outlying values. Directly below is a look at three of our selected beer styles (from left to right: American IPA, Irish Red Ale and American Light Lager) where we have created a box plot for each to visualize how spread the data is for each feature. As we can see, our data is heavily impacted by outlying values. This is something that we will want to remedy so that points that are particularly outside of the inter-quartile range do not adversely affect our analysis by giving those values too much weight.

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_raw_1.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_raw_2.png")), interpolate = FALSE)
img3 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_raw_3.png")), interpolate = FALSE)

grid.arrange(img1, img2, img3, nrow = 1)

```

## Data Transformation

Our solution to the outlying data issues was to scaled 2 standard deviations of our data between 0 and 1. This transformation allowed us to remove all outlying values that would overly affected our analysis and normalize between 0 and 1 so that no single feature with a large range would dominate the analysis. Please see below for the transformation on the previously displayed beer styles (from left to right: American IPA, Irish Red Ale and American Light Lager).

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_scale_1.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_scale_2.png")), interpolate = FALSE)
img3 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/boxplot_scale_3.png")), interpolate = FALSE)

grid.arrange(img1, img2, img3, nrow = 1)

```

# Our Analysis

## K Means

To begin our K Means analysis, we will begin by selecting notable features (based on our discretion); we have selected Color, ABV and IBU. We will then plot the natural clusters based on our cleaned data, to show the relation between each of these variables, based on the accompanying class label. Then, we will use R's kmeans() function with our data, specify that we are looking for ten clusters, and then visualize the same plots as above, however, in the K Means cases, we will color them observations based on the cluster they have been assigned to. In this way, we can compare the natural clusters against how K Means clusters the data.

### Color and ABV

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_nat_1.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_kmeans_1.png")), interpolate = FALSE)

grid.arrange(img1, img2, nrow = 1)

```

We draw the following notable conclusions:

* With the natural clustering, we see approximately five dominant clusters:
    * American Brown Ale - Light Brown
    * Irish Red Ale - Purple
    * American Light Lager - Green
    * Witbier - Magenta
    * Imperial IPA - Blue
* With K Means, we see approximately four dominant clusters:
    * Cluster 1 - Red
    * Cluster 2 - Light Brown
    * Cluster 7 - Blue
    * Cluster 9 - Light Purple
* Overall, we see much stronger, specific color density with the K Means results; we can infer this is because closer observations are lumped together in the same cluster
* It appears that Cluster 7 is a combination of American Brown Ale and the Irish Red Ale with a higher Color value
* Cluster 9 appears to be a combination of Witbier and the Irish Red Ale with a lower Color value

### IBU and Color

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_nat_2.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_kmeans_2.png")), interpolate = FALSE)

grid.arrange(img1, img2, nrow = 1)

```

We draw the following notable conclusions:

* With the natural clustering, we see approximately seven dominant clusters:
    * American Amber Ale - Red
    * American Brown Ale - Light Brown
    * Irish Red Ale - Purple
    * American Light Lager - Green
    * Witbier - Magenta
    * Saison - Light Purple
    * Imperial IPA - Blue
    * American IPA - Green-Brown
* With K Means, we see approximately three dominant clusters:
    * Cluster 2 - Light Brown
    * Cluster 7 - Blue
    * Cluster 9 - Light Purple
* We see very defined natural clusters, which is impressive because there are seven distinct ones; however, our clusters with K Means are much less strong
* It looks as though K Means transformed the natural cluster amalgam of American Light Lager and American Brown Ale and split them up evenly between two clusters (Clusters 2 and 9)
* In this case, we believe that K Means negatively impacted the natural clustering process; where there were seven very strongly defined clusters naturally, K Means created only three dominant clusters which are quite ill-defined

### ABV and IBU

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_nat_3.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/clust_kmeans_3.png")), interpolate = FALSE)

grid.arrange(img1, img2, nrow = 1)

```

We draw the following notable conclusions:

* With the natural clustering, we see approximately four dominant clusters:
    * Irish Red Ale - Purple
    * American Light Lager - Green
    * Witbier - Magenta
    * Imperial IPA - Blue
* With K Means, we see approximately three dominant clusters:
    * Cluster 2 - Light Brown
    * Cluster 7 - Blue
    * Cluster 9 - Light Purple
* Aside from Imperial IPA and Witbier, we dont't see impressive natural clustering; the large center cluster seems to be a mixture of Irish Red Ale, American Brown Ale and American Light Lager
* The K Means again did a poor clustering in this case; there appears to be the three clusters we defined (with Clusters 7 and 9 being very similar) and not much else
* There is clearly a one-to-one relationship between the Natural Cluster of Imperial IPA and K Means Cluster 2

### Classification Rate

To determine the K Means classification rate, we first determined how many of each Beer Style contributed to each of our contrived K Means clusters (columns 1-10 below); in this way, we can see which Beer Styles contribute the most to each Cluster. The table below shows us this result:

```{r, echo = FALSE}

kable(clust_table)

```

Using the above result, we can see the fraction of Beer Style contributions to each K Means Cluster. Taking it one step further, we can determine which Beer Style is the "winner" (drawing a parallel with the winning methodology used by SOM) of the corresponding Cluster based on the highest percent contributor to that cluster; and can then refer to the observations that map to these clusters of the winning Beer Style as correctly classified. The table below shows this percent result:

```{r, echo = FALSE}

kable(perc_clust)

```

Based on the above methodology, we calculated our Classfication Rate (correct classifications) as: 32.86%.

## Self-Organizing Map

### Network Parameters

* Network Parameters
    * Topology: 225 PEs (15x15 2-dimensional square lattice)
    * Each PE is a 10-dimensional vector (see below for weight draws)
* Learning Parameters
    * Initial Weights: Drawn from U[-0.5, 0.5]
    * Initial Learning Rate ($\alpha_{init}$): 0.3
    * Learning Decay Rate: $\alpha_{init} * e^\frac{-i}{n}$
        * i = current learning step
        * n = total number of learning steps
    * Initial Radius ($\theta_{init}$): 7.5
    * Radius Decay Rate: $\theta_{init} * e^\frac{-i}{\mu}$
        * i = current learning step
        * n = total number of learning steps
        * $\mu = \frac{n}{log(\theta_{init})}$
    * Momentum: None
    * Stopping Criteria: 2,500,000 learning steps
* Input Data
    * As previously described
* Error and Performance Measure
    * Learning Steps Performed: 2,500,000
    * Monitoring Frequency: Every 500,000 Learning Steps
    
### Decay Graphs

```{r, echo = FALSE, fig.height = 4, fig.align = 'center'}

img <- readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/ler_rate_decay.png")
grid.raster(img)

```

```{r, echo = FALSE, fig.height = 4, fig.align = 'center'}

img <- readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/radius_decay.png")
grid.raster(img)

```

### Learning Process - PE Density

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/pe_dens_500k.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/pe_dens_1m.png")), interpolate = FALSE)
img3 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/pe_dens_1.5m.png")), interpolate = FALSE)
img4 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/pe_dens_2m.png")), interpolate = FALSE)

grid.arrange(img1, img2, img3, img4, nrow = 2)

```

The above matrix of graphs represents the number of inputs that map to each of the PEs in our 15x15 network lattice at several learning steps. The images are arranged as follows:

* Top Left - 500,000 Learning Steps
* Top Right - 1,000,000 Learning Steps
* Bottom Left - 1,500,000 Learning Steps
* Bottom Right - 2,000,000 Learning Steps

As we can see, during the learning process, the number of mapped inputs to each PE increases over time, that is, as learning progresses, the inputs are mapped to a wider range of PEs, instead of just a few at the beginning of the learning process. We can see that at learning step 500,000, most of the inputs are mapped to a select few PEs mostly at the outside of the lattice, while there does appear to be a cluster forming in the bottom left area that is not part of the fringe of the lattice. At the 1,000,000 learning step, much of the inputs that were originally mapped to PEs on the outside of the lattice are now moving towards the center of the lattice; we can see a strong cluster forming near the middle and bottom of the lattice. At the 1,500,000 learning step, we are still seeing a strong favoritism of the inputs to map to PEs on the fringes of the lattice, however, the density in the middle of the lattice is starting to become much more uniform. Finally, at the 2,000,000 learning step, the lattice is almost completely uniform; this is a great result. We will show the final PE density latttice in our final SOM conclusion section; the lattice at the 2,500,000 learning step.

### Learning Process - PE Dimensionality

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/vec_plot_500k.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/vec_plot_1m.png")), interpolate = FALSE)
img3 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/vec_plot_1.5m.png")), interpolate = FALSE)
img4 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/vec_plot_2m.png")), interpolate = FALSE)

grid.arrange(img1, img2, img3, img4, nrow = 2)

```

The above matrix of graphs represents the vector of dimensions to each of the PEs in our 15x15 network lattice at several learning steps. That is, each PE is represented by a line that shows each of its dimensions in 10-dimensional space. In addition, each line is colored by the Beer Style for which most the inputs mapped to. So, for example, if most of the inputs that mapped to the lattice at (1, 1) were American IPA, we would color the drawn line red. The images are arranged as follows:

* Top Left - 500,000 Learning Steps
* Top Right - 1,000,000 Learning Steps
* Bottom Left - 1,500,000 Learning Steps
* Bottom Right - 2,000,000 Learning Steps

The colors for the lines are defined as follows according to the Beer Style:

* Red - American IPA
* Sandy - American Pale Ale
* Blue - American Light Lager
* Green - Saison
* Orange - American Amber Ale
* Purple - Imperial IPA
* Magenta - American Stout
* Turquoise - Irish Red Ale
* Dark Red - American Brown Ale
* Dark Turquoise - Witbier
* Black - No PE Mapped

At learning step 500,000 we can see that almost half of the PEs map to American IPA (Red), while another fraction (approximately one in five) map to American Pale Ale (Sandy). We can see some marginally substantial clusters starting to form, such as American Stout (Magenta) in the top left and American Amber Ale (Orange) in the bottom left. As we move from learning step 500,000 to learning step 1,000,000, we can see that American IPA (Red) is beginning to form at least two distinct subclasses in its right cluster, whereas there is no clear definition of its rightmost cluster. The Amber Ale (Orange) cluster has moved closer to the top where it is blending with a cluster formed from Irish Red Ale (Turquoise). A Saison (Green) cluster has formed near the bottom of the lattice trying to differentiate itself from the neighboring American IPA (Red) and American Pale Ale (Sandy). Moving from learning step 1,000,000 to 1,500,000, the structure of our large American IPA (Red) and American Pale Ale (Sandy) clusters remain the same, with each having many subclasses. Meanwhile, American Stout (Magenta) is solidifying its cluster in the west side of the lattice, competing with Irish Red Ale (Turquoise) and American Amber Ale (Orange). There are many distinct American Amber Ale (Orange) clusters forming across the lattice. Finally, moving onto the 2,000,000 learning step from learning step 1,500,000, we are seeing a second American Stout (Magenta) subclass forming that is quite distinct from the first. Otherwise, there weren't many drastic changes. We will show the final vector dimensionality latttice in our final SOM conclusion section; the lattice at the 2,500,000 learning step.

### Final Results

```{r, echo = FALSE}

img1 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/pe_dens_2.5m.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("~/Documents/Rice_University/Spring_2018/NML502/Final_Project/Plots/vec_plot_2.5m.png")), interpolate = FALSE)

grid.arrange(img1, img2, nrow = 1)

```

As we can see on the PE Density Plot, not much has changed since learning step 2,000,000; however, we are continuing to see the density per PE decreasing as inputs are continuing to be mapped to a wider variety of PEs.

As for the final PE dimensionality plot, we are seeing the following characteristics of our data:

* American IPA (Red) and American Pale Ale (Sandy) are very ill-defined, not only are their winning PEs spread out, but withing the connected clusters we are seeing several, often indistinct subclasses
* American Stout (Magenta) is one of the most well-defined clusters (on the left side of the lattice) with two apparent and distinct subclasses
* American Brown Ale (Dark Red) is also quite well defined, like American Stout (Magenta) it appears to have two reasonably distinct subclasses
* Saison (Green), while not having a true distinct cluster in the lattice space, does appear to have two reasonalby distinct subclasses (ignoring the PEs it won near the top and top-left of the lattice)
* Imperial IPA (Purple) also has a very well-defined cluster at the bottom left of the lattice; interstingly, it appears to have attracted an American IPA (Red) cluster, which makes sense since they are both the same style of beer

### Classification Rate

Our methodology for calculating the classification rate it quite simple and compares nicely with our methodology in the K Means case. Essentially, we find the winning Beer Type of each PE based on which Beer Type had the most inputs mapped to each PE. This will allow us to declare a class winner for each PE. Then, we sum all of the inputs that mapped to a PE that was won by their particular class and then divide by all of the inputs; we have the following classification rates at each monitoring frequency:

* Learning Step 500,000: 52.24%
* Learning Step 1,000,000: 53.53%
* Learning Step 1,500,000: 53.98%
* Learning Step 2,000,000: 56.09%
* Learning Step 2,500,000: 55.86%

# The Comparison

# Conclusion














